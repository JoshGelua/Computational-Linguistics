{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COG403: Problem 1 of Problem Set 2: Semantic Representations\n",
    "\n",
    "### All 3 problems for Problem Set 2 Due 1 November 2018, 2:00pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we will look at three different approaches for generating word vectors. We will then evaluate these word vectors on their ability to match human similarity judgments on a set of noun-noun pairs.  Specifically, we will use a subset of SimLex-999$^1$ that contains animal nouns and has average similarity judgments on pairs of these obtained from human participants.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "\n",
    "Write a function `generate_WN_vectors()` to generate feature vectors based on Word Net features$^2$. Generate these vectors and then save them in `data/word_net.vec` using the `write_vectors` function in `provided_functions.py`.\n",
    "\n",
    "Word net features are in the file `data/all_catf_norm_prob_lexicon_cs.all.txt`. The format is:\n",
    "    $$<word>:<part\\_of\\_speech>\\textrm{ }<feat1\\_name>:<feat1\\_val>,<feat2\\_name:feat2\\_val>,...$$\n",
    "\n",
    "Note that the feature names may contain spaces, which can be confusing, since the $<word>:<part\\_of\\_speech>$ is separated from the list of features by a space.\n",
    "\n",
    "Your approach for generating feature vectors for wordnet features should be a two-step process. First, collect all the features in the file `data/all_catf_norm_prob_lexicon_cs.all.txt` that occur with the words in `data/vocab.txt` (you can load the vocabulary using the `get_vocab` function defined in `provided_functions.py`). Then, generate feature vectors where each field in a vector corresponds to a feature, and the value in the field is the feature value. For example, if you found the features `['animal', 'mammal', 'carnivore']`, your vectors for *cat* and *rat* could be:\n",
    "\n",
    "* *cat*: `[0.18, 0.07, 0.02]`\n",
    "* *rat*: `[0.18, 0.07, 0]`\n",
    "\n",
    "Not all features will be listed for all animals. In these cases, set the field to be 0.0.\n",
    "\n",
    "Please print your vector for *cat*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.031851, 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "       0.      , 0.      , 0.180627, 0.130518, 0.130518, 0.100903,\n",
       "       0.09453 , 0.061783, 0.051288, 0.044947, 0.      , 0.073416,\n",
       "       0.071086, 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "       0.      , 0.      , 0.024253, 0.      , 0.      , 0.      ,\n",
       "       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "       0.      , 0.      , 0.      , 0.      , 0.002853, 0.001427,\n",
       "       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n",
       "       0.      ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import provided_functions as f \n",
    "vocabulary = f.get_vocab(\"data/vocab.txt\")\n",
    "length_of_vocab = len(vocabulary)\n",
    "def generate_WN_vectors():\n",
    "    with open(\"data/all_catf_norm_prob_lexicon_cs.all.txt\") as f:\n",
    "        text_to_list = []\n",
    "        for line in f:\n",
    "            line = line.split(\",\\n\")\n",
    "            if line[0] != \"\\n\":\n",
    "                text_to_list.append(line[0])\n",
    "    word_feature = []\n",
    "    for entry in text_to_list:\n",
    "        temp_list = []\n",
    "        entry = re.split(':[A-Z]*\\s', entry)\n",
    "        temp_list.append(entry[0])\n",
    "        temp_feature = re.split(\",\", entry[1])\n",
    "        temp_list.extend(temp_feature)\n",
    "        word_feature.append(temp_list)\n",
    "    vector_features = []\n",
    "    for word in word_feature:\n",
    "        if word[0] in vocabulary:\n",
    "            for i in range(1, len(word)):\n",
    "                feature_vector = re.split(\"[#0-9]*:\", word[i])\n",
    "                if feature_vector[0] not in vector_features:\n",
    "                    vector_features.append(feature_vector[0])\n",
    "    word_vector_dict = {}\n",
    "    length_of_features = len(vector_features)\n",
    "    for word in vector_features:\n",
    "        word_vector_dict[word] = np.zeros((length_of_features,), dtype = float)\n",
    "    for word in word_feature:\n",
    "        if word[0] in vocabulary:\n",
    "            for i in range(1, len(word)):\n",
    "                feature_vector = re.split(\"[#0-9]*:\", word[i])\n",
    "                word_vector_dict[word[0]][vector_features.index(feature_vector[0])] = feature_vector[1]\n",
    "                \n",
    "    return word_vector_dict\n",
    "\n",
    "WN_vectors = generate_WN_vectors()\n",
    "f.write_vectors(\"data/word_net.vec\", WN_vectors)\n",
    "WN_vectors['cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "\n",
    "Write a function `generate_SWOW_vectors()` to generate feature vectors based on Small World of Words$^3$ features. Generate these vectors and then save them in `data/small_world_of_words.vec` using the `write_vectors` function in `provided_functions.py`.\n",
    "\n",
    "The file `data/SWOW-EN.R100.csv` contains cues and responses for a free association task hosted online. In the task, the participant is shown a cue word and asked to respond by listing three response words.  (You can try this out yourself at http://www.smallworldofwords.org!)\n",
    "\n",
    "You should start by generating a zero-valued square matrix of cues by responses where the cues and responses are the words in `vocab.txt` (note: only save vectors for words in `vocab.txt` that occur as cues). Then read through the `data/SWOW-EN.R100.csv` file, and update the row in the matrix corresponding to each cue word, by adding 1 to the count of each response word that appears for that cue. For example, if your vocab (and thus the heading of your matrix columns) was `['cat', 'dog', 'goat', 'fluffy', 'kitten']`, and the first line in the data set had the cue *cat* and the responses [*dog*, *kitten*, *fluffy*], your updated vectors would look like this:\n",
    " * *cat*: [0, 1, 0, 1, 1]\n",
    " * *dog*: [0, 0, 0, 0, 0]\n",
    " * *goat*: [0, 0, 0, 0, 0]\n",
    " * *fluffy*: [0, 0, 0, 0, 0]\n",
    " * *kitten*: [0, 0, 0, 0, 0] \n",
    " \n",
    "After generating your cue-to-response vectors, set the diagonal entries of the matrix to be the highest value of each row; that is, the entry for the cue itself in the response vector of the cue should be set to the highest response count for that cue. This is done to make the vectors more appropriate for assessing similarity. We want the word *cat* to be the most similar to itself.\n",
    "\n",
    "For example, if we have the following vector for *cat* (assuming the same feature order as in the example above):\n",
    " * *cat*: [0, 30, 1, 20, 15],\n",
    " \n",
    "we would set the response count for *cat* to the max response count, 30, to get the following:\n",
    " * *cat*: [30, 30, 1, 20, 15]\n",
    "\n",
    "\n",
    "Please print your SWOW vector for *cat* as well as the value associated with *cat* in the cat vector. Make sure to label these two things clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The SWOW vector for cat is: \n",
      "[ 0  0  4  0  0 10 61  6  0  0  1  0 12  0  0  0  0  0  1  0  0  1  1  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 61  0  0  0  0  0  0  0\n",
      "  0  0]\n",
      " and the value associated with cat in the cat vector is:  61\n"
     ]
    }
   ],
   "source": [
    "import provided_functions as f \n",
    "import csv\n",
    "import numpy as np\n",
    "vocabulary = f.get_vocab(\"data/vocab.txt\")\n",
    "length_vocab = len(vocabulary)\n",
    "\n",
    "def generate_SWOW_vectors():\n",
    "    cues_dict = {}\n",
    "    for i in vocabulary:\n",
    "        cues_dict[i] = np.zeros((length_vocab,), dtype=int)\n",
    "    \n",
    "    with open('data/SWOW-EN.R100.csv', 'rt') as f:\n",
    "        reader = csv.reader(f)\n",
    "        SWOW_entries = list(reader)\n",
    "    cues_response = []\n",
    "    for index in SWOW_entries:\n",
    "        if index[-4] in vocabulary:\n",
    "            cues_response.append([index[-4], index[-3],index[-2],index[-1]])\n",
    "    \n",
    "    for cr in cues_response:\n",
    "        for index in range(1, len(cr)):\n",
    "            if cr[index] in vocabulary:\n",
    "                cues_dict[cr[0]][vocabulary.index(cr[index])] += 1\n",
    "    for key, value in cues_dict.items():\n",
    "        value[vocabulary.index(key)] = np.amax(value)\n",
    "    return cues_dict\n",
    "\n",
    "            \n",
    "cues_dict = generate_SWOW_vectors()\n",
    "cat_vector = cues_dict[\"cat\"]\n",
    "cat_value = cues_dict[\"cat\"][vocabulary.index(\"cat\")]\n",
    "print (\"The SWOW vector for cat is: \\n\"\n",
    "       +str(cat_vector)\n",
    "       +\"\\n and the value associated with cat in the cat vector is: \",\n",
    "       str(cat_value))\n",
    "f.write_vectors(\"data/small_world_of_words.vec\", cues_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "\n",
    "Write a function `evaluate_vectors` that takes a set of vectors and evaluates them using SimLex-999. Write your function according to the description below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "SIM_LEX_FILE = 'data/SimLex-999-Animals.txt'\n",
    "\n",
    "\n",
    "def evaluate_vectors(word_vector_dict):\n",
    "    \"\"\"\n",
    "    word_vector_dict: dict of str to list -- a dictionary mapping words to vectors.\n",
    "    \n",
    "    1. Iterate over the word pairs in the SimLex-999 Animal subset at `data/SimLex-999-Animals.txt`.\n",
    "       Compute the similarities between your word vectors for the word pairs using cosine similarity\n",
    "       (use `sklearn.metrics.pairwise.cosine_similarity`). Skip word pairs where one or both of the\n",
    "       vectors are missing from word_vector_dict.\n",
    "    2. Print the pearson correlation r, and the corresponding p value, between the SimLex-999\n",
    "       similarity scores and the cosine similarities for word pairs. (Use `scipy.stats.pearsonr`).\n",
    "       Please print \"Pearson r is: \", and \"with p value: \".\n",
    "    3. Use `matplotlib` to generate a plot of SimLex-999 similarity scores and cosine similarities\n",
    "       for word pairs.\n",
    "    \"\"\"\n",
    "    simlex_sim_scores = []\n",
    "    cos_sim_scores = np.array([])\n",
    "    with open (SIM_LEX_FILE, \"rt\") as f:\n",
    "        for line in f:\n",
    "            simlex = line.split(\"\\t\")\n",
    "            if ((simlex[0] in word_vector_dict) and (simlex[1] in word_vector_dict) ):\n",
    "                simlex_sim_scores.append(simlex[2].rstrip())\n",
    "                word1 = word_vector_dict[simlex[0]]\n",
    "                word2 = word_vector_dict[simlex[1]]\n",
    "                word1 = np.array(word1,dtype=float)\n",
    "                word2 = np.array(word2,dtype=float)\n",
    "                cos_sim = cosine_similarity([word1], [word2])\n",
    "                if cos_sim > 0.9:\n",
    "                    print (simlex[0], simlex[1], simlex[2])\n",
    "                cos_sim_scores = np.concatenate((cos_sim_scores,cos_sim),axis=None)\n",
    "    simlex_sim_scores = (np.float_(simlex_sim_scores))\n",
    "    correlation_coefficient, p_value = pearsonr(simlex_sim_scores, cos_sim_scores)      \n",
    "    print (\"Pearson r is: \" + str(correlation_coefficient) + \" with p value: \" + str(p_value))\n",
    "    \n",
    "    plt.scatter(simlex_sim_scores, cos_sim_scores)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)\n",
    "\n",
    "Run `evaluate_vectors` from part c on the WordNet vectors generated in part a. Use `load_vectors` defined in `provided_functions.py` to load the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cow cattle 9.52\n",
      "\n",
      "mouse cat 1.12\n",
      "\n",
      "rat mouse 7.78\n",
      "\n",
      "cat lion 6.75\n",
      "\n",
      "bird turkey 6.58\n",
      "\n",
      "insect bee 6.07\n",
      "\n",
      "sheep cattle 4.77\n",
      "\n",
      "cow goat 2.93\n",
      "\n",
      "bee queen 3.27\n",
      "\n",
      "bee ant 2.78\n",
      "\n",
      "cat rabbit 2.37\n",
      "\n",
      "bird hen 7.03\n",
      "\n",
      "chicken steak 3.73\n",
      "\n",
      "hen turkey 6.13\n",
      "\n",
      "Pearson r is: 0.27187074663931066 with p value: 0.18861558125165806\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import provided_functions as f \n",
    "WN_vec = f.load_vectors(\"data/word_net.vec\")\n",
    "evaluate_vectors(WN_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e)\n",
    "\n",
    "Run `evaluate_vectors` from part c on the Small World of Words vectors generated in part b. Use `load_vectors` defined in `provided_vectors.py` to load the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horse mare 8.33\n",
      "\n",
      "horse colt 7.07\n",
      "\n",
      "bird hawk 7.85\n",
      "\n",
      "hound fox 2.38\n",
      "\n",
      "mink fur 6.83\n",
      "\n",
      "bee ant 2.78\n",
      "\n",
      "oil mink 1.23\n",
      "\n",
      "daughter kid 4.17\n",
      "\n",
      "chicken steak 3.73\n",
      "\n",
      "horse ox 3.02\n",
      "\n",
      "calf bull 4.93\n",
      "\n",
      "dog horse 2.38\n",
      "\n",
      "mouse management 0.48\n",
      "\n",
      "deck mouse 0.48\n",
      "\n",
      "container mouse 0.3\n",
      "\n",
      "Pearson r is: 0.30020703341442356 with p value: 0.08451655032349062\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import provided_functions as f\n",
    "SWOW_vec = f.load_vectors('data/small_world_of_words.vec')\n",
    "evaluate_vectors(SWOW_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f)\n",
    "\n",
    "Run `evaluate_vectors` from part c on word2vec vectors stored in `data/word2vec.vec`. Use `load_vectors` defined in `provided_functions.py` to load the vectors.\n",
    "\n",
    "These vectors are a subset of vectors from a 2017 paper by Fares$^4$ with:\n",
    " * dimension: 300\n",
    " * window: 5\n",
    " * corpus: Gigaword 5th Edition\n",
    " * vocab size: 261794\n",
    " * algorithm: Gensim Continuous Skipgram\n",
    " * lemmatization: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e63fcb82febc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprovided_functions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mW2_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/word2vec.vec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mevaluate_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW2_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate_vectors' is not defined"
     ]
    }
   ],
   "source": [
    "import provided_functions as f\n",
    "W2_vec = f.load_vectors('data/word2vec.vec')\n",
    "evaluate_vectors(W2_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (g)\n",
    "\n",
    " * 1. Compare the performance of the three vector spaces referring to both the results of your Pearson correlations and the scatterplots.\n",
    "\n",
    " * 2. For each features space, examine some example similarity comparisons and the corresponding word vectors. From your inspection, what do you think is responsible for the different performance of the vector spaces in this task? Show example noun-noun pairs that support your hypothesis about why each of the vector spaces performs well or does not perform well.  Explain how these examples support your hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1. <br/>\n",
    "The performance goes as follows:\n",
    "\n",
    "* For d) data points for these vector spaces from word_net is skewed. Many points within the graph are uniformly at the top. A pearson correlation performance is hindered by these points. At a p-value of 0.19~, we can reject the alternative hypothesis that there is a correlation of r = 0.27~. Therefore, there is no correlation significant at the 90% or 95% confidence level. \n",
    "* For e), unlike d), the data points is skewed as many points within the correlation graph are uniformly at the bottom. Generally, the scatter plot appears as though there is a correlation. Given by the values of pearsonr, we can see this is not the case at the 95% confidence level as the p-value = 0.08451655032349062. However, I suspect outliers and zero-datapoints heavily affect the correlation coefficient and p-value. Such datapoints skew the data, and in removing them we may better see a p-value and correlation coefficient more reflective of the data. At the 90% confidence level, however, we may say that there is a positive correlation between simlex-999 and the cosine similarity scores of SWOW vectors at r = 0.3~. Such a correlation is statistically significant at the 90% level.\n",
    "* For f), unlike d) and e), has many datapoints available, and the data generally consistent with a correlation (e.g. no obvious outliers, no zero-points). Since the p-value is 0.0033110382715266892, at the 95% confidence level, we may conclude that data in word2vec has a significant correlation with simlex-999 data points, and the correlation coefficient is positive at r = 0.4893719058835222~.\n",
    "<br\\>\n",
    "* 2. \n",
    "Likewise from the previous answer, such a disprepancy in the vector space in WordNet has a large affect on the performance in this task. Some vectors, such as for mouse and cat have almost all data point that matches and therefore has a cosine similarity score close to 1. However, it has a simlex-99 score of 1.12. Such vectors affect the graph and skew the regression and ultimately hinders the performance of the model. \n",
    "In another case, outliers and zero cos_sim scores heavily influence the performance of the vector spaces. For example, horse and mare have a <0.1 cosine similarity score, lower than most Noun-Noun pairs in this data. Yet, simlex-999 scores it a 8.33. Ultimately, the zero-points of cosine similarity affects the performance. Such datapoints skew the regression of the data and ultimately hinders the performance of the model.\n",
    "In the W2_vec vector space, the datapoints and their cosine similarity scores are generally consistent with the rest of the data, i.e. no outliers or zero-scores. The data available is ultimately more than the WordNet and SWOW vectors. It regresses into a positive correlation at the p-value of 0.0033110382715266892, less than 0.05 level that allows us to come to the conclusion that such correlation of simlex-999 and cosine similarity scores of W2_vec is statistically significant at the 95% confidence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordNet Nouns example\n",
      "Mouse:\n",
      "['0.032359' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.183507'\n",
      " '0.132599' '0.132599' '0.102512' '0.096037' '0.062768' '0.052105'\n",
      " '0.045664' '0.0' '0.074587' '0.072219' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.011595' '0.001449'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0\\n']\n",
      "Cat:\n",
      "['0.048045' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.272463' '0.0'\n",
      " '0.0' '0.152206' '0.142592' '0.093195' '0.077363' '0.067799' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0538' '0.045192' '0.038736' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.006456' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.002152' '0.0\\n']\n",
      "SWOW Nouns example\n",
      "Horse:\n",
      "['0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '9' '1' '0' '0' '0' '0' '9' '0' '0' '0' '4' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0\\n']\n",
      "Mare:\n",
      "['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0\\n']\n",
      "Word2Vec Nouns example\n",
      "Dog:\n",
      "['-0.187956' '-0.576374' '-0.099378' '0.381505' '-0.034920' '0.662993'\n",
      " '0.403795' '-0.504661' '0.271683' '0.664742' '0.572801' '0.391702'\n",
      " '-0.175632' '0.334175' '-0.418716' '0.880376' '-0.776999' '-0.236448'\n",
      " '-0.437179' '-0.402898' '0.605225' '0.206453' '0.649173' '-0.020535'\n",
      " '0.195228' '0.435480' '-0.356695' '-0.297003' '0.753823' '0.381901'\n",
      " '-0.023558' '0.116041' '0.165225' '-1.328727' '-0.235878' '-0.186595'\n",
      " '0.233233' '0.502537' '-0.169190' '0.450573' '-0.293062' '-0.262067'\n",
      " '0.070613' '0.016534' '0.536001' '-0.063618' '-0.563297' '0.950493'\n",
      " '-0.198283' '0.173649' '0.235086' '-0.458623' '-0.309464' '-0.461100'\n",
      " '0.097848' '-0.115201' '-0.183937' '0.220529' '-0.441323' '0.074265'\n",
      " '0.154289' '0.072733' '0.183437' '-0.199054' '0.440639' '-0.171841'\n",
      " '0.650229' '-0.539144' '-0.423035' '-0.048725' '-0.322653' '-0.270446'\n",
      " '0.567587' '0.308086' '-0.202252' '0.208237' '0.038470' '-0.492895'\n",
      " '0.432774' '-0.614658' '-0.416508' '0.006910' '-0.396625' '-0.437460'\n",
      " '-0.623182' '0.809147' '-0.282259' '-0.001973' '0.149887' '-0.037521'\n",
      " '0.245675' '0.608327' '0.015966' '-0.414597' '-0.489306' '-0.546594'\n",
      " '0.172616' '0.101673' '0.091192' '0.573523' '0.078577' '0.175116'\n",
      " '-0.189040' '0.153342' '-0.165493' '-0.429415' '0.107025' '-0.106172'\n",
      " '-0.084381' '-0.032291' '-0.075368' '0.289532' '0.260989' '0.369629'\n",
      " '-0.210340' '0.486072' '0.263833' '0.536831' '-0.769980' '0.198002'\n",
      " '0.108243' '0.545785' '0.227985' '-0.036271' '-0.443213' '0.218040'\n",
      " '0.202533' '0.358055' '0.710656' '0.392202' '0.489327' '0.531588'\n",
      " '-0.772029' '0.633901' '0.395475' '0.334173' '0.493336' '0.105454'\n",
      " '0.065573' '0.250915' '-0.188365' '0.393228' '-0.068369' '0.051130'\n",
      " '0.150347' '0.357629' '0.011406' '0.276524' '-0.227215' '-0.125641'\n",
      " '-0.036376' '-0.698272' '0.538157' '0.191639' '0.239891' '0.277397'\n",
      " '0.384187' '0.491824' '0.137515' '1.030566' '0.305299' '-0.153995'\n",
      " '0.178249' '-0.662220' '0.147328' '-0.151256' '-0.317225' '-0.447727'\n",
      " '0.589411' '-0.259636' '0.013431' '0.445777' '-0.182228' '0.354243'\n",
      " '0.043861' '0.088199' '0.497814' '0.023080' '-0.555684' '-0.516628'\n",
      " '0.337557' '0.311753' '-0.640268' '-0.563423' '-0.103659' '-0.443959'\n",
      " '-0.060210' '-0.066899' '0.103711' '-0.123077' '0.652969' '0.434432'\n",
      " '0.412643' '0.560435' '0.348187' '-0.056629' '0.175321' '-0.162380'\n",
      " '-0.952616' '0.343147' '-0.602855' '0.147280' '0.286469' '-0.575492'\n",
      " '0.468590' '-0.717664' '0.029476' '-0.452041' '0.459889' '0.227545'\n",
      " '0.204830' '-0.204705' '-0.226857' '0.695111' '0.678019' '-0.311193'\n",
      " '-0.367845' '0.141134' '-0.656574' '0.150177' '-0.239690' '-0.216816'\n",
      " '0.331253' '-0.764553' '0.560034' '-0.289125' '0.725344' '0.189100'\n",
      " '-0.169837' '-1.180877' '-0.219115' '0.625226' '-0.234790' '-0.068018'\n",
      " '-0.051659' '-0.380299' '0.371037' '0.274763' '-0.381641' '0.517303'\n",
      " '-1.136787' '1.142170' '-0.167943' '-0.760565' '0.282648' '0.048736'\n",
      " '0.247873' '0.849435' '0.765334' '0.131228' '0.520798' '0.867567'\n",
      " '-0.508811' '-0.332433' '-0.505514' '-0.085914' '0.220988' '-0.446214'\n",
      " '-0.252348' '-0.090061' '0.054504' '-0.427272' '0.056685' '-0.249915'\n",
      " '0.455869' '-1.264609' '0.413469' '0.354768' '0.412730' '-0.792996'\n",
      " '-0.056604' '0.331234' '0.270079' '0.031969' '0.310917' '0.095629'\n",
      " '-0.986801' '-0.460804' '-0.073847' '-0.670419' '-0.043895' '-0.384747'\n",
      " '-0.002839' '-0.173417' '-0.300731' '-0.191447' '-0.552234' '0.271966'\n",
      " '-0.111534' '-0.219022' '0.006802' '-0.274981' '0.007225' '0.145600'\n",
      " '0.202521' '0.280714' '0.256343' '-0.467481' '0.475499' '-0.218778\\n']\n",
      "Cat:\n",
      "['-0.030510' '-0.219258' '0.208356' '0.605209' '0.031824' '0.539372'\n",
      " '0.831034' '-0.498476' '0.348149' '0.482086' '0.973080' '-0.051308'\n",
      " '-0.424477' '0.370969' '0.082189' '1.075413' '-0.186794' '-0.713830'\n",
      " '-0.257695' '-0.599751' '0.522187' '0.198099' '0.143058' '0.065377'\n",
      " '0.076064' '-0.095881' '-0.227032' '-0.310105' '0.200591' '0.268745'\n",
      " '-0.113465' '0.232583' '0.265311' '-0.416211' '0.253842' '0.070479'\n",
      " '0.483618' '0.387476' '0.051666' '0.232553' '-0.459484' '-0.173974'\n",
      " '-0.025405' '0.155713' '0.402326' '0.124108' '0.190185' '0.823629'\n",
      " '0.537462' '0.314799' '0.380077' '-0.284195' '-0.357232' '-0.206095'\n",
      " '0.360582' '0.022357' '-0.379062' '-0.103901' '-0.230969' '0.138418'\n",
      " '-0.297476' '-0.138301' '0.249721' '-0.287777' '0.218165' '-0.718574'\n",
      " '1.049481' '-0.322890' '-0.074914' '-0.155123' '0.086249' '0.373635'\n",
      " '0.004990' '0.409668' '0.112650' '-0.003822' '-0.126497' '-0.817465'\n",
      " '0.387146' '-0.590701' '-0.191143' '0.265951' '-0.271256' '0.104922'\n",
      " '-0.266301' '0.577893' '-0.164821' '0.213611' '-0.000028' '0.144063'\n",
      " '0.620689' '0.342102' '0.297409' '0.044080' '-0.500889' '-0.791786'\n",
      " '0.042862' '0.057525' '-0.466203' '1.106704' '-0.052683' '0.629375'\n",
      " '0.017494' '0.041805' '-0.160238' '-0.244396' '0.240456' '-0.222708'\n",
      " '0.261083' '0.205089' '-0.461600' '0.360231' '0.232016' '-0.290220'\n",
      " '0.157973' '-0.055750' '0.086785' '0.069648' '-0.513269' '0.031227'\n",
      " '-0.356844' '-0.099294' '0.826368' '-0.415003' '-0.192453' '0.212814'\n",
      " '-0.520959' '0.480930' '0.649174' '0.790247' '0.697772' '0.343829'\n",
      " '-0.020461' '0.193717' '0.406501' '0.875910' '0.160783' '0.431040'\n",
      " '-0.293468' '0.193610' '-0.496300' '0.376313' '-0.115967' '0.067954'\n",
      " '0.026157' '0.206465' '-0.300431' '0.111386' '-0.135749' '-0.486364'\n",
      " '0.222474' '-0.773868' '0.824136' '0.207038' '0.326333' '0.672248'\n",
      " '0.501042' '-0.304307' '0.375674' '0.242500' '-0.089511' '-0.567449'\n",
      " '0.263215' '-0.338777' '-0.237821' '-0.257867' '-0.024144' '0.049573'\n",
      " '0.737534' '-0.160888' '0.069165' '0.010884' '0.223232' '0.447325'\n",
      " '0.173929' '0.634444' '0.191132' '0.290159' '-0.109394' '-0.394099'\n",
      " '0.413886' '0.016523' '-0.432952' '-0.133243' '-0.065065' '-0.175450'\n",
      " '-0.110246' '0.063655' '0.483706' '0.219550' '0.724847' '0.260669'\n",
      " '0.545532' '-0.105634' '0.527781' '-0.140979' '0.335768' '0.262027'\n",
      " '-0.729536' '-0.064289' '-0.549974' '-0.001336' '0.670889' '-0.201478'\n",
      " '0.071257' '-0.247750' '-0.185479' '-0.003455' '0.757716' '0.321316'\n",
      " '-0.203066' '-0.351589' '0.606485' '0.611296' '0.274556' '0.015465'\n",
      " '-0.092076' '-0.132292' '-0.345324' '-0.009838' '0.082957' '-0.227774'\n",
      " '-0.028869' '-0.383367' '-0.041273' '-0.791424' '0.520078' '-0.015747'\n",
      " '0.228250' '-0.399343' '-0.563174' '0.360613' '0.059697' '-0.226602'\n",
      " '0.100663' '-0.257228' '0.635062' '0.754916' '-0.038834' '0.334358'\n",
      " '-1.224287' '0.524054' '-0.190877' '-0.372278' '0.079715' '0.182402'\n",
      " '0.037008' '0.856984' '0.876060' '0.186854' '0.108800' '-0.016716'\n",
      " '-0.511060' '-0.093950' '-0.065025' '-0.079481' '0.328206' '-0.296119'\n",
      " '-0.061437' '-0.061705' '-0.010320' '-0.213708' '-0.159285' '-0.137934'\n",
      " '0.023403' '-0.718247' '0.226487' '0.459930' '-0.263499' '-0.280784'\n",
      " '-0.290351' '0.415764' '-0.021996' '-0.502458' '0.392624' '-0.200775'\n",
      " '-0.704582' '-0.238976' '-0.343274' '-0.461438' '-0.071727' '-0.066764'\n",
      " '0.070002' '-0.272428' '-0.064685' '0.058221' '-0.585899' '0.664546'\n",
      " '-0.068528' '-0.627903' '0.382968' '-0.936954' '-0.289952' '0.080468'\n",
      " '0.422128' '0.216063' '0.019948' '-0.278022' '0.529659' '-0.488361\\n']\n"
     ]
    }
   ],
   "source": [
    "WN_vec = f.load_vectors(\"data/word_net.vec\")\n",
    "print (\"WordNet Nouns example\")\n",
    "print (\"Mouse:\")\n",
    "print (WN_vec[\"mouse\"])\n",
    "print (\"Cat:\")\n",
    "print (WN_vec[\"ant\"])\n",
    "SWOW_vec = f.load_vectors('data/small_world_of_words.vec')\n",
    "print (\"SWOW Nouns example\")\n",
    "print (\"Horse:\")\n",
    "print (SWOW_vec[\"horse\"])\n",
    "print (\"Mare:\")\n",
    "print (SWOW_vec[\"mare\"])\n",
    "W2_vec = f.load_vectors('data/word2vec.vec')\n",
    "print (\"Word2Vec Nouns example\")\n",
    "print (\"Dog:\")\n",
    "print (W2_vec[\"dog\"])\n",
    "print (\"Cat:\")\n",
    "print (W2_vec[\"cat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations\n",
    "\n",
    "$^1$Hill, F., Reichart, R., & Korhonen, A. (2015). SimLex-999: Evaluating Semantic Models With (Genuine) Similarity Estimation. Computational Linguistics, 41(4), 665-695. doi:10.1162/coli_a_00237\n",
    "\n",
    "$^2$Miller, G. A., Beckwith, R., Fellbaum, C., Gross, D., & Miller, K. J. (1990). Introduction to WordNet: An On-line Lexical Database*. International Journal of Lexicography, 3(4), 235-244. doi:10.1093/ijl/3.4.235\n",
    "\n",
    "$^3$Data is from http://www.smallworldofwords.org.\n",
    "\n",
    "$^4$ Fares, Murhaf; Kutuzov, Andrei; Oepen, Stephan & Velldal, Erik (2017). Word vectors, reuse, and replicability: Towards a community repository of large-text resources, In JÃ¶rg Tiedemann (ed.), Proceedings of the 21st Nordic Conference on Computational Linguistics, NoDaLiDa, 22-24 May 2017. LinkÃ¶ping University Electronic Press. ISBN 978-91-7685-601-7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
